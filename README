SUMMARY
-------

apiary is a distributed, protocol-independent load testing framework, written
in Python, that replays captured queries, simulating production load patterns.

A QueenBee process feeds sequences of one or more messages into an AMQP queue,
and one or more WorkerBee processes retrieve sequences from the queue, send the
associated messages to the specified target host, and report their results to
the BeeKeeper to be tallied.  The QueenBee prints a summary of the progress
every 15 seconds and when all the WorkerBees are done.


REQUIREMENTS
------------

* RabbitMQ 1.6

* lsprof (if you want to use --profile)
http://codespeak.net/svn/user/arigo/hack/misc/lsprof/

* py-amqplib (only tested with an implementation of the 0-8 spec)
http://hg.barryp.org/py-amqplib/file/

* maatkit or percona-toolkit (either may be available in your Linux distribution)
http://www.percona.com/software/percona-toolkit


RABBITMQ SETUP
--------------

To configure a local, running instance of RabbitMQ, execute the following:

    sudo sh bin/setup-rabbitmq.sh

This will delete the apiary vhost and user, re-add them, and then set up
the appropriate permissions.  This must be run as root.


SAMPLE OUTPUT
-------------

To see sample output from apiary, you can run apiary in --no-mysql mode.

First, start up some worker bees:

  $ bin/apiary -f 10 --protocol mysql --no-mysql

Next, run the queen bee:

  $ bin/apiary --queenbee --protocol mysql doc/examples/example-good-sequences.log

Apiary will pretend to run the queries in the sequence file, and after a few tens
of seconds, you'll see the sample output.  The --no-mysql switch causes the
worker bees not to connect to MySQL and to return '200 OK' responses.

If you don't have RabbitMQ running on localhost, you'll need to specify --amqp-*
options on both the worker bee and queen bee command lines.  Run bin/apiary --help
for documentation on these options.

TUTORIAL
--------

This tutorial assumes you have mysql running on localhost with no password for
the root user.  Adapt the commands below appropriately if that is not the case.

1. Some sample data is included.  This will create an "apiary_demo" database:

    mysql -u root < data/demo.sql

2. Use tcpdump to capture mysql traffic for maatkit consumption:

    sudo tcpdump -i lo port 3306 -s 65535 -x -n -q -tttt> /tmp/tcpdump.out

3. In a separate terminal, run a script that will generate some queries against
the test data:

    bin/run-demo-queries.sh

4. Stop the tcpdump process that you started in step 2.
5. Turn the tcpdump data into a query digest using maatkit or percona-toolkit:

    mk-query-digest --type=tcpdump --no-report --print /tmp/tcpdump.out > /tmp/apiary_query_digest.txt

Use pt-query-digest if you have percona-toolkit instead of maatkit.  They both work identically.

6. Convert the query digest into a sequence file:

    PYTHONPATH=. python apiary/mysql/genseqs.py /tmp/apiary_query_digest.txt > /tmp/apiary_seq.txt

7. Run some worker bees:

    bin/apiary -f 10 --protocol mysql --mysql-host localhost --mysql-user root

8. Run the queen bee:

    bin/apiary --queenbee --protocol mysql /tmp/apiary_seq.txt

9. The queen bee will read the query sequences and push them into the worker-job queue.  The worker
   bees will execute the jobs and return status back to the beekeeper running inside the queenbee process.

10. You should see a summary of the results in a few seconds. 
