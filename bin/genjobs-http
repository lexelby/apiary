#!/usr/bin/env python

import sys
import os.path
import re
import cPickle
import time
from dateutil.parser import parse as parse_datetime
from dateutil.tz import tzutc
from datetime import datetime
from pprint import pprint


UTC = tzutc()

def unix_time(dt):
    """Why is there no function for this?"""
    epoch = datetime.utcfromtimestamp(0).replace(tzinfo=UTC)
    delta = dt - epoch
    return delta.total_seconds()


# Packets were captured with gulp (http://staff.washington.edu/corey/gulp/).
# TCP flows parsed and dumped by tcpflow (https://github.com/simsong/tcpflow)
# using this command: tcpflow -r foo.pcap -I -FT -Fc -Fg -o flows
# which produces a directory tree containing flows, two files per flow.  One
# file contains the data, and one contains the timing data.  A summary file,
# report.xml, lists all connections, almost though not completely in
# chronological order.

FILENAME_RE = re.compile('<filename>([^<]+)</filename>')
START_TIME_RE = re.compile("<tcpflow startime='([^']+)'")


def parse_report(report_path):
    report = open(report_path)

    files = []

    current_file_name = None

    for line in report:
        match = FILENAME_RE.search(line)
        if match:
            current_file_name = match.group(1)
        else:
            match = START_TIME_RE.search(line)
            if match and current_file_name:
                start_time = match.group(1)

                files.append((start_time, current_file_name))

                current_file_name = None

    return files


def is_request_start(data):
    return data.startswith('GET /') or data.startswith('POST /')


def parse_flow(flow_path, start_timestamp):
    """Disassemble a flow of HTTP requests given sent data and temporal index.

    This relies heavily on the fact that requests in our traffic flow seem to
    always start and end on packet boundaries.  It's possible that the start (or
    even the middle or end) of an HTTP request was missed in the capture.

    This is a really basic algorithm that makes a lot of assumptions.  It's
    conceivably possible that, for example, a request could contain a verbatim
    "GET /" in it somewhere and that that text might line up on a packet
    boundary.  In that case, our request flow will be corrupted, but this is
    rare enough that the overall load test will probably not be noticeably
    impacted.
    """

    requests = []

    data = open(flow_path)
    index = open(flow_path + ".findx")

    current_request = None
    current_request_timestamp = None
    in_request = False

    for line in index:
        offset, timestamp, length = line.split('|')
        timestamp = float(timestamp) - start_timestamp

        packet = data.read(int(length))

        if is_request_start(packet):
            # A new request is starting.

            # If we already had a request going, store it.
            if in_request:
                requests.append((current_request_timestamp, current_request))
            else:
                in_request = True

            current_request = packet
            current_request_timestamp = timestamp
        else:
            if in_request:
                current_request += packet

    if in_request:
        # Finish off the last request.
        requests.append((current_request_timestamp, current_request))
    else:
        #print >> sys.stderr, "never found the start of a request in", flow_path
        pass

    return requests


def main():
    report = sys.argv[1]
    jobs_file = open(sys.argv[2], 'w')

    print "reading file list"

    files = sorted(parse_report(report))
    num_files = len(files)

    print "...done, %d files found" % num_files

    print "generating jobs"

    job_id = 0
    num_requests = 0
    num_bogus = 0
    start_timestamp = unix_time(parse_datetime(files[0][0]))

    for i, flow in enumerate(item[1] for item in files):
        if i % 1000 == 0:
            print "%d files processed, %0.2f%% complete (%d bogus files)" % (i, float(i) * 100 / num_files, num_bogus)

        requests = parse_flow(flow, start_timestamp)

        if requests:
            num_requests += len(requests)

            cPickle.dump((job_id, tuple(requests)), jobs_file)
            job_id += 1
        else:
            num_bogus += 1

    print "created %d jobs containing %d total requests (%d bogus files)" % (job_id + 1, num_requests, num_bogus)

if __name__ == '__main__':
    sys.exit(main())
